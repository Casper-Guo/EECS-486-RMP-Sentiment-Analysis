{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0559590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tkinter as tk  \n",
    "from tkinter import ttk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36550b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guoca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\guoca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\guoca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\guoca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad9a38",
   "metadata": {},
   "source": [
    "# Ingest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be05b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv(\"Data/clean_ratings.csv\", infer_datetime_format=True)\n",
    "\n",
    "df_profs = pd.read_csv(\"Data/clean_prof_info.csv\")\n",
    "df_profs[\"firstName\"] = df_profs[\"firstName\"].apply(lambda x: x.strip())\n",
    "df_profs[\"lastName\"] = df_profs[\"lastName\"].apply(lambda x: x.strip())\n",
    "\n",
    "df_names = df_profs[[\"profID\", \"firstName\", \"lastName\"]]\n",
    "df_ratings = df_ratings.merge(df_names, how=\"inner\", on=\"profID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "874f257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "stopword_list = set(stopwords.words(\"english\"))\n",
    "stopword_list.update([',', '.'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_comment(row):\n",
    "    \"\"\"\n",
    "    Tokenize, remove stopwords and punctuations at word ends, lemmatize, and then reassemble into one string.\n",
    "    \n",
    "    If any token matches the first or last name of the professor, it is dropped.\n",
    "    \n",
    "    All numbers are dropped.\n",
    "    \n",
    "    This is done to eliminate low-impact tokens and reduce vocabulary size.\n",
    "    \n",
    "    String type output required for easier ingestion by sklearn TfidfVectorizer.\n",
    "    \"\"\"\n",
    "    comment = row.loc[\"comment\"]\n",
    "    re.sub(r\"['!\\\"#$%&\\'()*,./:;<=>?@[\\\\]^_`{|}~'] \", ' ', comment)\n",
    "    tokens = word_tokenize(comment)\n",
    "    \n",
    "    ignore_list = stopword_list.copy()\n",
    "    ignore_list.update([row.loc[\"firstName\"], row.loc[\"lastName\"]])\n",
    "    \n",
    "    tokens = [token.lower() for token in tokens if token not in ignore_list and not token.isnumeric()]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85af0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings[\"comment\"] = df_ratings.apply(preprocess_comment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46b77e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings[\"Hot\"] = df_ratings[\"helpfulRating\"] >= 4\n",
    "df_ratings[\"Hot\"] = df_ratings[\"Hot\"].astype(int)\n",
    "\n",
    "X, y = df_ratings[\"comment\"], df_ratings[\"Hot\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1bad1",
   "metadata": {},
   "source": [
    "# Train Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce74eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(strip_accents=&#x27;ascii&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(strip_accents=&#x27;ascii&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(strip_accents='ascii')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(strip_accents=\"ascii\")\n",
    "tfidf_vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a16421",
   "metadata": {},
   "source": [
    "# Load Trained Models\n",
    "\n",
    "Unable to recover XGBoost model, needs retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9488a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guoca\\anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.23.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\guoca\\anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.23.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\guoca\\anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.23.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = load(\"Models/log_reg_tfidf.joblib\")\n",
    "random_forest = load(\"Models/rf_tfidf.joblib\")\n",
    "svm = load(\"Models/svm_tfidf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32120fa7",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be89a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29151753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(learning_rate=0.5, n_estimators=200, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.5, n_estimators=200, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.5, n_estimators=200, random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier as XGBoost\n",
    "\n",
    "# Hyperparameters selected by prior grid search\n",
    "xgboost = XGBoost(learning_rate=0.5, n_estimators=200, random_state=42)\n",
    "xgboost.fit(train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a11f52f",
   "metadata": {},
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dfd5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class demo_gui:\n",
    "    def __init__(self, root):\n",
    "        \n",
    "        root.title(\"Interactive Demo\")\n",
    "\n",
    "        mainframe = ttk.Frame(root, padding=\"3 3 12 12\")\n",
    "        mainframe.grid(column=0, row=0, sticky=(tk.N, tk.W, tk.E, tk.S))\n",
    "        root.columnconfigure(0, weight=1)\n",
    "        root.rowconfigure(0, weight=1)\n",
    "\n",
    "        self.text = tk.StringVar()\n",
    "        text_entry = ttk.Entry(mainframe, width=100, textvariable=self.text)\n",
    "        text_entry.grid(column=2, row=1, sticky=(tk.W, tk.E))\n",
    "\n",
    "        ttk.Button(mainframe, text=\"Predict\", command=self.predict).grid(column=3, row=1, sticky=(tk.W, tk.E))\n",
    "\n",
    "        ttk.Label(mainframe, text=\"Enter a comment\").grid(column=1, row=1, sticky=(tk.W, tk.E))\n",
    "        ttk.Label(mainframe, text=\"Processed Comment\").grid(column=1, row=2, sticky=(tk.W, tk.E))\n",
    "        ttk.Label(mainframe, text=\"Random Forest\").grid(column=1, row=3, sticky=(tk.W, tk.E))\n",
    "        ttk.Label(mainframe, text=\"XGBoost\").grid(column=1, row=4, sticky=(tk.W, tk.E))\n",
    "        ttk.Label(mainframe, text=\"Logistic Regression\").grid(column=1, row=5, sticky=(tk.W, tk.E))\n",
    "        ttk.Label(mainframe, text=\"Support Vector Machine\").grid(column=1, row=6, sticky=(tk.W, tk.E))\n",
    "        \n",
    "        self.tokens = tk.StringVar()\n",
    "        ttk.Label(mainframe, textvariable=self.tokens).grid(column=2, row=2, sticky=(tk.W, tk.E))\n",
    "        \n",
    "        self.rf_pred = tk.StringVar()\n",
    "        ttk.Label(mainframe, textvariable=self.rf_pred).grid(column=2, row=3, sticky=(tk.W, tk.E))\n",
    "        \n",
    "        self.xgb_pred = tk.StringVar()\n",
    "        ttk.Label(mainframe, textvariable=self.xgb_pred).grid(column=2, row=4, sticky=(tk.W, tk.E))\n",
    "\n",
    "        self.lgr_pred = tk.StringVar()\n",
    "        ttk.Label(mainframe, textvariable=self.lgr_pred).grid(column=2, row=5, sticky=(tk.W, tk.E))\n",
    "\n",
    "        self.svm_pred = tk.StringVar()\n",
    "        ttk.Label(mainframe, textvariable=self.svm_pred).grid(column=2, row=6, sticky=(tk.W, tk.E))\n",
    "        \n",
    "        for child in mainframe.winfo_children(): \n",
    "            child.grid_configure(padx=10, pady=5)\n",
    "\n",
    "        text_entry.focus()\n",
    "    \n",
    "    def predict(self):\n",
    "        tokens, matrix = self.process_input()\n",
    "        \n",
    "        if not tokens:\n",
    "            self.tokens.set('')\n",
    "            self.rf_pred.set('')\n",
    "            self.xgb_pred.set('')\n",
    "            self.lgr_pred.set('')\n",
    "            self.svm_pred.set('')\n",
    "            \n",
    "            return\n",
    "        \n",
    "        self.tokens.set(tokens)\n",
    "        self.rf_pred.set(self.random_forest_predict(random_forest, matrix))\n",
    "        self.xgb_pred.set(self.xgboost_predict_proba(xgboost, matrix))\n",
    "        self.lgr_pred.set(self.logistic_regression_predict(logistic_regression, matrix))\n",
    "        self.svm_pred.set(self.support_vector_machine_predict(svm, matrix))\n",
    "    \n",
    "\n",
    "    def process_input(self):\n",
    "        text = self.text.get()\n",
    "        re.sub(r\"['!\\\"#$%&\\'()*,./:;<=>?@[\\\\]^_`{|}~'] \", ' ', text)\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        tokens = [token.lower() for token in tokens if token not in stopword_list and not token.isnumeric()]\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens = ' '.join(tokens)\n",
    "\n",
    "        return tokens, tfidf_vectorizer.transform([tokens])\n",
    "    \n",
    "    \n",
    "    def random_forest_predict(self, clf, encoding):\n",
    "        pred = clf.predict_proba(encoding)\n",
    "        \n",
    "        return f\"Hot probability: {round(pred[0, 1], 2)}, not hot probability: {round(pred[0, 0], 2)}\"\n",
    "    \n",
    "    \n",
    "    def logistic_regression_predict(self, clf, encoding):\n",
    "        pred = clf.predict_proba(encoding)\n",
    "        \n",
    "        return f\"Hot probability: {round(pred[0, 1], 2)}, not hot probability: {round(pred[0, 0], 2)}\"\n",
    "    \n",
    "    \n",
    "    def support_vector_machine_predict(self, clf, encoding):\n",
    "        pred = clf.predict(encoding)\n",
    "        hot_or_not = \"Hot\" if pred[0] else \"Not hot\"\n",
    "        \n",
    "        return f\"Predicted class: {hot_or_not}\"\n",
    "    \n",
    "    \n",
    "    def xgboost_predict_proba(self, clf, encoding):\n",
    "        pred = clf.predict_proba(encoding)\n",
    "        \n",
    "        return f\"Hot probability: {round(pred[0, 1], 2)}, not hot probability: {round(pred[0, 0], 2)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc8e6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = tk.Tk()\n",
    "demo_gui(root)\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
